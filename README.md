Universidade Federal do Rio Grande do Norte
Bacharelado em Ci√™ncia e Tecnologia 

Disciplina: Projedo de Sistemas Baseados em Aprendizado de M√°quinas

# Atividade CIFAR-100
üìä AN√ÅLISE DOS RESULTADOS - CIFAR-100


## Observa√ß√µes sobre o Treinamento com CIFAR-100:

A LeNet-5 adaptada para CIFAR-100 (com in_channels=3 e 100 classes de sa√≠da) foi treinada por
24 √©pocas, demonstrando progress√£o consistente mas limitada devido √† complexidade do dataset.

*Resultados Alcan√ßados:*
- Acur√°cia inicial (Epoch 1): Train 1.15% | Test 1.20%
- Acur√°cia final (Epoch 24): Train 23.55% | Test *23.08%*
- Loss final: Train 3.1849 | Test 3.2323
- Progress√£o total: ganho de ~20% em acur√°cia ao longo de 24 √©pocas

Essa acur√°cia de 23.08% √© condizente com o esperado para a arquitetura LeNet-5 aplicada ao
CIFAR-100. O dataset CIFAR-100 √© significativamente mais desafiador que o CIFAR-10 (que atinge
65-70% com LeNet-5), devido ao n√∫mero 10x maior de classes (100 vs 10).

As curvas de loss e accuracy revelam:
- *Converg√™ncia gradual*: redu√ß√£o consistente do loss ao longo das √©pocas
- *Overfitting moderado*: pequena diferen√ßa entre train loss (3.08) e test loss (3.21)
- *Aprendizado cont√≠nuo*: acur√°cia de teste ainda estava melhorando na √©poca 24
- *Limita√ß√£o arquitetural*: LeNet-5 tem apenas 69,656 par√¢metros, insuficientes para
  capturar toda a complexidade de 100 classes distintas

## An√°lise dos Feature Maps:

A visualiza√ß√£o dos feature maps revela o processamento hier√°rquico da informa√ß√£o:

- *conv1 + relu1*: Os 6 filtros da primeira camada (28x28) detectam caracter√≠sticas de baixo
  n√≠vel como bordas horizontais, verticais, diagonais e gradientes de cor. Com 100 classes,
  a rede precisa identificar detalhes sutis que diferenciam categorias similares.

- *pool1*: O max pooling (2x2) reduz de 28x28 para 14x14, mantendo as caracter√≠sticas mais
  salientes e criando invari√¢ncia posicional parcial, essencial para robustez.

- *conv2 + relu2*: Os 16 filtros da segunda camada (10x10) combinam features de conv1 para
  detectar padr√µes mais complexos e abstratos. Observa-se ativa√ß√£o seletiva para estruturas
  espec√≠ficas das imagens.

- *pool2*: Segunda redu√ß√£o dimensional (14x14 -> 5x5), consolidando representa√ß√µes abstratas
  antes do classificador.

A visualiza√ß√£o confirma que a LeNet-5, embora simples, consegue extrair features relevantes.
Por√©m, para 100 classes, arquiteturas mais profundas (VGG, ResNet, EfficientNet) seriam mais
adequadas, pois oferecem maior capacidade de representa√ß√£o (milh√µes de par√¢metros vs 69k).

## Compara√ß√£o e Contexto:

- *CIFAR-10 com LeNet-5*: ~65-70% de acur√°cia (esperado)
- *CIFAR-100 com LeNet-5*: ~23.08% de acur√°cia (nosso resultado)
- *CIFAR-100 com ResNet-18*: ~75-80% de acur√°cia (benchmark comum)
- *CIFAR-100 State-of-the-art*: >95% de acur√°cia (redes modernas)

A diferen√ßa de 3x na dificuldade (70% -> 23%) ao aumentar de 10 para 100 classes demonstra
o desafio de classifica√ß√£o granular com arquiteturas simples.

## Conclus√£o:

O experimento demonstrou com sucesso:

‚úÖ Implementa√ß√£o de CNN (LeNet-5) adaptada para CIFAR-100

‚úÖ Treinamento completo de 24 √©pocas com registro de m√©tricas

‚úÖ Implementa√ß√£o de hooks para captura de ativa√ß√µes intermedi√°rias

‚úÖ Visualiza√ß√£o de feature maps de todas as 6 camadas

‚úÖ An√°lise quantitativa e qualitativa dos resultados

Os resultados confirmam que a complexidade do CIFAR-100 (100 classes) exige arquiteturas mais
sofisticadas para atingir acur√°cias superiores a 70-80%. A LeNet-5, com sua simplicidade,
atingiu 23.08% - um resultado razo√°vel considerando suas limita√ß√µes arquiteturais.


